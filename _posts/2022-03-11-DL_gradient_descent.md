# Gridient Descent


Here's the table of contents:

1. TOC
{:toc}

## Overview
Gradient descent is by far the most crucial and the cornerstone of deep learning and it is fair to say that deep learning would not be possible without it. Gradient descent plays a fundamental role in optimization theory, where it is used to minimize a function by gradient descent or to maximize it by gradient ascent. Using the concept of gradient descent is essential to minimizing the error of the model in deep learning.

The dradient of a function is closely related to its derivative. 
Basically, the gradient of a function is the dual of its derivative, or in other words,  From the calculus we know that a minimum of a function happens in the point in which the function's derivative is  equal to zero.
