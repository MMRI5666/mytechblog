# Gradient Descent


Here's the table of contents:

1. TOC
{:toc}

## Overview
Gradient descent is by far the most crucial mathematical concept used in deep learning algorithms, and it is fair to say that deep learning would not be possible without it. Gradient descent plays a fundamental role in optimization theory, where it is used to minimize or maximize a function by gradient descent or gradient ascent respectively. Gradient descent is essential to minimizing the error of a deep learning model.

The dradient of a function is closely related to its derivative. 
Basically, the gradient of a function <em>f</em> is the dual to its <em>total derivative</em>, or in other words, they are related in that the <em>dot product</em? of the gradient of <em>f</em> at the point <em>p</em> with another tangent vector <em>v</em> equals the <em>directional derivative</em> of <em>f</em>, which is depicted as below:

From the calculus we know that a minimum of a function happens in the point in which the function's derivative is  equal to zero.
